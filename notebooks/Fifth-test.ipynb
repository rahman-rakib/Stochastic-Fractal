{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, numpy as np, pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "alphap = 2\n",
    "probp = 0.75\n",
    "expon = 2 * alphap - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def betadist(alpha):\n",
    "    \"\"\"gives a random number from beta distribution\"\"\"\n",
    "    return random.betavariate(alpha,alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision(probability):\n",
    "    \"\"\"\n",
    "    decides with a given probability whether to keep the right part\n",
    "    \"\"\"\n",
    "    if float(probability) > random.random():\n",
    "        return True\n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting(left,right):\n",
    "    \"\"\"\n",
    "    splits a given segment. left and right are endpoints of the segment\n",
    "    \"\"\"\n",
    "    segment = right - left\n",
    "    xL = segment * betadist(alphap)\n",
    "    xR = segment - xL\n",
    "    splitpoint = left + xL\n",
    "    flag = decision(probp)\n",
    "    xLp = xL**expon\n",
    "    xRp = xR**expon\n",
    "    change = xLp + xRp - segment**expon\n",
    "    return splitpoint, flag, xLp, xRp, change\n",
    "\n",
    "def splitting_v2(segment):\n",
    "    \"\"\"\n",
    "    splits a given segment. left and right are endpoints of the segment\n",
    "    returns : \n",
    "        xL -> length of the left segment\n",
    "        xR -> length of the right segment\n",
    "        flag -> keeping the right segment\n",
    "        xLp, xRp -> probability(unnormalized) for being selected\n",
    "        change -> change of normalization const\n",
    "    \"\"\"\n",
    "    xL = segment * betadist(alphap)\n",
    "    xR = segment - xL\n",
    "    flag = decision(probp)\n",
    "    xLp = xL**expon\n",
    "    xRp = xR**expon\n",
    "    change = xLp + xRp - segment**expon\n",
    "    return xL, xR, flag, xLp, xRp, change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickindex(expons, exponsum):\n",
    "    \"\"\"\n",
    "    picks up a segment to be subsequently split\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, exponsum)\n",
    "    sum_ = 0\n",
    "    for index in range(len(expons)):\n",
    "        sum_ += expons[index]\n",
    "        if sum_ < r:\n",
    "            continue\n",
    "        else:\n",
    "            return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_length(points,flags):\n",
    "    \n",
    "    N = 0\n",
    "    M = 0\n",
    "    \n",
    "    for i in range(len(flags)):\n",
    "        if flags[i]:\n",
    "            N += 1\n",
    "            M += points[i+1] - points[i] \n",
    "            pass\n",
    "        pass\n",
    "    return N, M\n",
    "\n",
    "def number_length_v2(lengths,flags):\n",
    "    \n",
    "    N = 0\n",
    "    M = 0\n",
    "#     print(flags)\n",
    "#     print(lengths)\n",
    "    for i in range(len(flags)):\n",
    "        if flags[i]:\n",
    "            N += 1\n",
    "            M += lengths[i]\n",
    "            pass\n",
    "        pass\n",
    "    return N, M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def realization_value(total_iteration, min_iteration, iteration_step):\n",
    "    \n",
    "    points = [0.,1.]\n",
    "    flags = [True]\n",
    "    expons = [1.]\n",
    "    exponsum = 1.0\n",
    "\n",
    "    iteration_list = list(range(min_iteration, total_iteration + 1, iteration_step))\n",
    "    N_realization = []\n",
    "    M_realization = []\n",
    "    \n",
    "    for i in range(total_iteration + 1):\n",
    "        \n",
    "        index = pickindex(expons, exponsum)\n",
    "        \n",
    "        if flags[index] == True:\n",
    "            left = points[index]\n",
    "            right = points[index+1]\n",
    "            splitpoint, flag, xLp, xRp, change = splitting(left,right)\n",
    "            points.insert(index+1,splitpoint)\n",
    "            flags.insert(index+1,flag)\n",
    "            expons[index] = xLp \n",
    "            expons.insert(index+1,xRp)\n",
    "            exponsum += change\n",
    "            pass\n",
    "        \n",
    "        if i+1 in iteration_list:\n",
    "            N, M = number_length(points,flags)\n",
    "            N_realization.append(N)\n",
    "            M_realization.append(M)\n",
    "        pass\n",
    "    \n",
    "    N_list = np.array(N_realization)\n",
    "    M_list = np.array(M_realization)\n",
    "    \n",
    "    return N_list, M_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def realization_value_v2(total_iteration, min_iteration, iteration_step):\n",
    "    \n",
    "    #points = [0.,1.]\n",
    "    lengths = [1.]\n",
    "    flags = [True]\n",
    "    probabilities = [1.] # raw probability. not normalized\n",
    "    normC = 1.0 # normalization const\n",
    "\n",
    "    iteration_list = list(range(min_iteration, total_iteration + 1, iteration_step))\n",
    "    N_realization = []\n",
    "    M_realization = []\n",
    "    \n",
    "    for i in range(total_iteration + 1):\n",
    "        \n",
    "        index = pickindex(probabilities, normC)\n",
    "        \n",
    "        if flags[index] == True:\n",
    "\n",
    "            xL, xR, flag, xLp, xRp, change = splitting_v2(lengths[index])\n",
    "            \n",
    "            lengths[index] = xL\n",
    "            lengths.append(xR)\n",
    "            flags.append(flag)\n",
    "            probabilities[index] = xLp \n",
    "            probabilities.append(xRp)\n",
    "            normC += change\n",
    "            pass\n",
    "        \n",
    "        if i+1 in iteration_list:\n",
    "            N, M = number_length_v2(lengths,flags)\n",
    "            N_realization.append(N)\n",
    "            M_realization.append(M)\n",
    "        pass\n",
    "    \n",
    "    N_list = np.array(N_realization)\n",
    "    M_list = np.array(M_realization)\n",
    "    \n",
    "    return N_list, M_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157 ms ± 21.1 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# realization_value_v2(10, 2, 2)\n",
    "%timeit realization_value_v2(100000, 50000, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 5.18 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "671 ms ± 348 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit realization_value(100000, 50000, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_average(total_iteration = 1000, min_iteration = 100, iteration_step = 100, ensemble_size = 1000):\n",
    "\n",
    "    data_points = int ((total_iteration - min_iteration)/iteration_step + 1)\n",
    "    N_ensemble = np.zeros(data_points)\n",
    "    M_ensemble = np.zeros(data_points)\n",
    "    \n",
    "    for i in range(ensemble_size):\n",
    "        if i % 100 == 0:\n",
    "            print(\"iteration \", i)\n",
    "        #N_list, M_list = realization_value(total_iteration, min_iteration, iteration_step)\n",
    "        N_list, M_list = realization_value_v2(total_iteration, min_iteration, iteration_step)\n",
    "        N_ensemble += N_list\n",
    "        M_ensemble += M_list\n",
    "        pass\n",
    "    \n",
    "    N_average = N_ensemble/ensemble_size\n",
    "    M_average = M_ensemble/ensemble_size\n",
    "    \n",
    "    return N_average, M_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  0\n",
      "iteration  100\n",
      "iteration  200\n",
      "iteration  300\n",
      "iteration  400\n",
      "iteration  500\n",
      "iteration  600\n",
      "iteration  700\n",
      "iteration  800\n",
      "iteration  900\n",
      "0.7788405112543598\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_average, M_average = ensemble_average(10_000, 1_000, 1_000, 1_000)\n",
    "N_log = np.log(N_average)\n",
    "minus_delta_log = N_log - np.log(M_average)\n",
    "slope, intercept = np.polyfit(minus_delta_log, N_log, 1)\n",
    "print(slope)\n",
    "plt.plot(minus_delta_log, N_log, \"o\")\n",
    "plt.xlabel(\"-ln delta\")\n",
    "plt.ylabel(\"ln N\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f():\n",
    "    arr = [   ]\n",
    "    for a in range(1000):\n",
    "        arr.insert(0, a)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323 µs ± 6.71 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g():\n",
    "    arr = [0]*1000\n",
    "    i = 1000-1\n",
    "    for a in range(1000):\n",
    "        arr[i] = a\n",
    "        i -= 1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.2 µs ± 7.1 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit g()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.310077519379845"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "685/129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([999, 998, 997, 996, 995, 994, 993, 992, 991, 990, 989, 988, 987,\n",
       "       986, 985, 984, 983, 982, 981, 980, 979, 978, 977, 976, 975, 974,\n",
       "       973, 972, 971, 970, 969, 968, 967, 966, 965, 964, 963, 962, 961,\n",
       "       960, 959, 958, 957, 956, 955, 954, 953, 952, 951, 950, 949, 948,\n",
       "       947, 946, 945, 944, 943, 942, 941, 940, 939, 938, 937, 936, 935,\n",
       "       934, 933, 932, 931, 930, 929, 928, 927, 926, 925, 924, 923, 922,\n",
       "       921, 920, 919, 918, 917, 916, 915, 914, 913, 912, 911, 910, 909,\n",
       "       908, 907, 906, 905, 904, 903, 902, 901, 900, 899, 898, 897, 896,\n",
       "       895, 894, 893, 892, 891, 890, 889, 888, 887, 886, 885, 884, 883,\n",
       "       882, 881, 880, 879, 878, 877, 876, 875, 874, 873, 872, 871, 870,\n",
       "       869, 868, 867, 866, 865, 864, 863, 862, 861, 860, 859, 858, 857,\n",
       "       856, 855, 854, 853, 852, 851, 850, 849, 848, 847, 846, 845, 844,\n",
       "       843, 842, 841, 840, 839, 838, 837, 836, 835, 834, 833, 832, 831,\n",
       "       830, 829, 828, 827, 826, 825, 824, 823, 822, 821, 820, 819, 818,\n",
       "       817, 816, 815, 814, 813, 812, 811, 810, 809, 808, 807, 806, 805,\n",
       "       804, 803, 802, 801, 800, 799, 798, 797, 796, 795, 794, 793, 792,\n",
       "       791, 790, 789, 788, 787, 786, 785, 784, 783, 782, 781, 780, 779,\n",
       "       778, 777, 776, 775, 774, 773, 772, 771, 770, 769, 768, 767, 766,\n",
       "       765, 764, 763, 762, 761, 760, 759, 758, 757, 756, 755, 754, 753,\n",
       "       752, 751, 750, 749, 748, 747, 746, 745, 744, 743, 742, 741, 740,\n",
       "       739, 738, 737, 736, 735, 734, 733, 732, 731, 730, 729, 728, 727,\n",
       "       726, 725, 724, 723, 722, 721, 720, 719, 718, 717, 716, 715, 714,\n",
       "       713, 712, 711, 710, 709, 708, 707, 706, 705, 704, 703, 702, 701,\n",
       "       700, 699, 698, 697, 696, 695, 694, 693, 692, 691, 690, 689, 688,\n",
       "       687, 686, 685, 684, 683, 682, 681, 680, 679, 678, 677, 676, 675,\n",
       "       674, 673, 672, 671, 670, 669, 668, 667, 666, 665, 664, 663, 662,\n",
       "       661, 660, 659, 658, 657, 656, 655, 654, 653, 652, 651, 650, 649,\n",
       "       648, 647, 646, 645, 644, 643, 642, 641, 640, 639, 638, 637, 636,\n",
       "       635, 634, 633, 632, 631, 630, 629, 628, 627, 626, 625, 624, 623,\n",
       "       622, 621, 620, 619, 618, 617, 616, 615, 614, 613, 612, 611, 610,\n",
       "       609, 608, 607, 606, 605, 604, 603, 602, 601, 600, 599, 598, 597,\n",
       "       596, 595, 594, 593, 592, 591, 590, 589, 588, 587, 586, 585, 584,\n",
       "       583, 582, 581, 580, 579, 578, 577, 576, 575, 574, 573, 572, 571,\n",
       "       570, 569, 568, 567, 566, 565, 564, 563, 562, 561, 560, 559, 558,\n",
       "       557, 556, 555, 554, 553, 552, 551, 550, 549, 548, 547, 546, 545,\n",
       "       544, 543, 542, 541, 540, 539, 538, 537, 536, 535, 534, 533, 532,\n",
       "       531, 530, 529, 528, 527, 526, 525, 524, 523, 522, 521, 520, 519,\n",
       "       518, 517, 516, 515, 514, 513, 512, 511, 510, 509, 508, 507, 506,\n",
       "       505, 504, 503, 502, 501, 500, 499, 498, 497, 496, 495, 494, 493,\n",
       "       492, 491, 490, 489, 488, 487, 486, 485, 484, 483, 482, 481, 480,\n",
       "       479, 478, 477, 476, 475, 474, 473, 472, 471, 470, 469, 468, 467,\n",
       "       466, 465, 464, 463, 462, 461, 460, 459, 458, 457, 456, 455, 454,\n",
       "       453, 452, 451, 450, 449, 448, 447, 446, 445, 444, 443, 442, 441,\n",
       "       440, 439, 438, 437, 436, 435, 434, 433, 432, 431, 430, 429, 428,\n",
       "       427, 426, 425, 424, 423, 422, 421, 420, 419, 418, 417, 416, 415,\n",
       "       414, 413, 412, 411, 410, 409, 408, 407, 406, 405, 404, 403, 402,\n",
       "       401, 400, 399, 398, 397, 396, 395, 394, 393, 392, 391, 390, 389,\n",
       "       388, 387, 386, 385, 384, 383, 382, 381, 380, 379, 378, 377, 376,\n",
       "       375, 374, 373, 372, 371, 370, 369, 368, 367, 366, 365, 364, 363,\n",
       "       362, 361, 360, 359, 358, 357, 356, 355, 354, 353, 352, 351, 350,\n",
       "       349, 348, 347, 346, 345, 344, 343, 342, 341, 340, 339, 338, 337,\n",
       "       336, 335, 334, 333, 332, 331, 330, 329, 328, 327, 326, 325, 324,\n",
       "       323, 322, 321, 320, 319, 318, 317, 316, 315, 314, 313, 312, 311,\n",
       "       310, 309, 308, 307, 306, 305, 304, 303, 302, 301, 300, 299, 298,\n",
       "       297, 296, 295, 294, 293, 292, 291, 290, 289, 288, 287, 286, 285,\n",
       "       284, 283, 282, 281, 280, 279, 278, 277, 276, 275, 274, 273, 272,\n",
       "       271, 270, 269, 268, 267, 266, 265, 264, 263, 262, 261, 260, 259,\n",
       "       258, 257, 256, 255, 254, 253, 252, 251, 250, 249, 248, 247, 246,\n",
       "       245, 244, 243, 242, 241, 240, 239, 238, 237, 236, 235, 234, 233,\n",
       "       232, 231, 230, 229, 228, 227, 226, 225, 224, 223, 222, 221, 220,\n",
       "       219, 218, 217, 216, 215, 214, 213, 212, 211, 210, 209, 208, 207,\n",
       "       206, 205, 204, 203, 202, 201, 200, 199, 198, 197, 196, 195, 194,\n",
       "       193, 192, 191, 190, 189, 188, 187, 186, 185, 184, 183, 182, 181,\n",
       "       180, 179, 178, 177, 176, 175, 174, 173, 172, 171, 170, 169, 168,\n",
       "       167, 166, 165, 164, 163, 162, 161, 160, 159, 158, 157, 156, 155,\n",
       "       154, 153, 152, 151, 150, 149, 148, 147, 146, 145, 144, 143, 142,\n",
       "       141, 140, 139, 138, 137, 136, 135, 134, 133, 132, 131, 130, 129,\n",
       "       128, 127, 126, 125, 124, 123, 122, 121, 120, 119, 118, 117, 116,\n",
       "       115, 114, 113, 112, 111, 110, 109, 108, 107, 106, 105, 104, 103,\n",
       "       102, 101, 100,  99,  98,  97,  96,  95,  94,  93,  92,  91,  90,\n",
       "        89,  88,  87,  86,  85,  84,  83,  82,  81,  80,  79,  78,  77,\n",
       "        76,  75,  74,  73,  72,  71,  70,  69,  68,  67,  66,  65,  64,\n",
       "        63,  62,  61,  60,  59,  58,  57,  56,  55,  54,  53,  52,  51,\n",
       "        50,  49,  48,  47,  46,  45,  44,  43,  42,  41,  40,  39,  38,\n",
       "        37,  36,  35,  34,  33,  32,  31,  30,  29,  28,  27,  26,  25,\n",
       "        24,  23,  22,  21,  20,  19,  18,  17,  16,  15,  14,  13,  12,\n",
       "        11,  10,   9,   8,   7,   6,   5,   4,   3,   2,   1,   0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(f())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([999, 998, 997, 996, 995, 994, 993, 992, 991, 990, 989, 988, 987,\n",
       "       986, 985, 984, 983, 982, 981, 980, 979, 978, 977, 976, 975, 974,\n",
       "       973, 972, 971, 970, 969, 968, 967, 966, 965, 964, 963, 962, 961,\n",
       "       960, 959, 958, 957, 956, 955, 954, 953, 952, 951, 950, 949, 948,\n",
       "       947, 946, 945, 944, 943, 942, 941, 940, 939, 938, 937, 936, 935,\n",
       "       934, 933, 932, 931, 930, 929, 928, 927, 926, 925, 924, 923, 922,\n",
       "       921, 920, 919, 918, 917, 916, 915, 914, 913, 912, 911, 910, 909,\n",
       "       908, 907, 906, 905, 904, 903, 902, 901, 900, 899, 898, 897, 896,\n",
       "       895, 894, 893, 892, 891, 890, 889, 888, 887, 886, 885, 884, 883,\n",
       "       882, 881, 880, 879, 878, 877, 876, 875, 874, 873, 872, 871, 870,\n",
       "       869, 868, 867, 866, 865, 864, 863, 862, 861, 860, 859, 858, 857,\n",
       "       856, 855, 854, 853, 852, 851, 850, 849, 848, 847, 846, 845, 844,\n",
       "       843, 842, 841, 840, 839, 838, 837, 836, 835, 834, 833, 832, 831,\n",
       "       830, 829, 828, 827, 826, 825, 824, 823, 822, 821, 820, 819, 818,\n",
       "       817, 816, 815, 814, 813, 812, 811, 810, 809, 808, 807, 806, 805,\n",
       "       804, 803, 802, 801, 800, 799, 798, 797, 796, 795, 794, 793, 792,\n",
       "       791, 790, 789, 788, 787, 786, 785, 784, 783, 782, 781, 780, 779,\n",
       "       778, 777, 776, 775, 774, 773, 772, 771, 770, 769, 768, 767, 766,\n",
       "       765, 764, 763, 762, 761, 760, 759, 758, 757, 756, 755, 754, 753,\n",
       "       752, 751, 750, 749, 748, 747, 746, 745, 744, 743, 742, 741, 740,\n",
       "       739, 738, 737, 736, 735, 734, 733, 732, 731, 730, 729, 728, 727,\n",
       "       726, 725, 724, 723, 722, 721, 720, 719, 718, 717, 716, 715, 714,\n",
       "       713, 712, 711, 710, 709, 708, 707, 706, 705, 704, 703, 702, 701,\n",
       "       700, 699, 698, 697, 696, 695, 694, 693, 692, 691, 690, 689, 688,\n",
       "       687, 686, 685, 684, 683, 682, 681, 680, 679, 678, 677, 676, 675,\n",
       "       674, 673, 672, 671, 670, 669, 668, 667, 666, 665, 664, 663, 662,\n",
       "       661, 660, 659, 658, 657, 656, 655, 654, 653, 652, 651, 650, 649,\n",
       "       648, 647, 646, 645, 644, 643, 642, 641, 640, 639, 638, 637, 636,\n",
       "       635, 634, 633, 632, 631, 630, 629, 628, 627, 626, 625, 624, 623,\n",
       "       622, 621, 620, 619, 618, 617, 616, 615, 614, 613, 612, 611, 610,\n",
       "       609, 608, 607, 606, 605, 604, 603, 602, 601, 600, 599, 598, 597,\n",
       "       596, 595, 594, 593, 592, 591, 590, 589, 588, 587, 586, 585, 584,\n",
       "       583, 582, 581, 580, 579, 578, 577, 576, 575, 574, 573, 572, 571,\n",
       "       570, 569, 568, 567, 566, 565, 564, 563, 562, 561, 560, 559, 558,\n",
       "       557, 556, 555, 554, 553, 552, 551, 550, 549, 548, 547, 546, 545,\n",
       "       544, 543, 542, 541, 540, 539, 538, 537, 536, 535, 534, 533, 532,\n",
       "       531, 530, 529, 528, 527, 526, 525, 524, 523, 522, 521, 520, 519,\n",
       "       518, 517, 516, 515, 514, 513, 512, 511, 510, 509, 508, 507, 506,\n",
       "       505, 504, 503, 502, 501, 500, 499, 498, 497, 496, 495, 494, 493,\n",
       "       492, 491, 490, 489, 488, 487, 486, 485, 484, 483, 482, 481, 480,\n",
       "       479, 478, 477, 476, 475, 474, 473, 472, 471, 470, 469, 468, 467,\n",
       "       466, 465, 464, 463, 462, 461, 460, 459, 458, 457, 456, 455, 454,\n",
       "       453, 452, 451, 450, 449, 448, 447, 446, 445, 444, 443, 442, 441,\n",
       "       440, 439, 438, 437, 436, 435, 434, 433, 432, 431, 430, 429, 428,\n",
       "       427, 426, 425, 424, 423, 422, 421, 420, 419, 418, 417, 416, 415,\n",
       "       414, 413, 412, 411, 410, 409, 408, 407, 406, 405, 404, 403, 402,\n",
       "       401, 400, 399, 398, 397, 396, 395, 394, 393, 392, 391, 390, 389,\n",
       "       388, 387, 386, 385, 384, 383, 382, 381, 380, 379, 378, 377, 376,\n",
       "       375, 374, 373, 372, 371, 370, 369, 368, 367, 366, 365, 364, 363,\n",
       "       362, 361, 360, 359, 358, 357, 356, 355, 354, 353, 352, 351, 350,\n",
       "       349, 348, 347, 346, 345, 344, 343, 342, 341, 340, 339, 338, 337,\n",
       "       336, 335, 334, 333, 332, 331, 330, 329, 328, 327, 326, 325, 324,\n",
       "       323, 322, 321, 320, 319, 318, 317, 316, 315, 314, 313, 312, 311,\n",
       "       310, 309, 308, 307, 306, 305, 304, 303, 302, 301, 300, 299, 298,\n",
       "       297, 296, 295, 294, 293, 292, 291, 290, 289, 288, 287, 286, 285,\n",
       "       284, 283, 282, 281, 280, 279, 278, 277, 276, 275, 274, 273, 272,\n",
       "       271, 270, 269, 268, 267, 266, 265, 264, 263, 262, 261, 260, 259,\n",
       "       258, 257, 256, 255, 254, 253, 252, 251, 250, 249, 248, 247, 246,\n",
       "       245, 244, 243, 242, 241, 240, 239, 238, 237, 236, 235, 234, 233,\n",
       "       232, 231, 230, 229, 228, 227, 226, 225, 224, 223, 222, 221, 220,\n",
       "       219, 218, 217, 216, 215, 214, 213, 212, 211, 210, 209, 208, 207,\n",
       "       206, 205, 204, 203, 202, 201, 200, 199, 198, 197, 196, 195, 194,\n",
       "       193, 192, 191, 190, 189, 188, 187, 186, 185, 184, 183, 182, 181,\n",
       "       180, 179, 178, 177, 176, 175, 174, 173, 172, 171, 170, 169, 168,\n",
       "       167, 166, 165, 164, 163, 162, 161, 160, 159, 158, 157, 156, 155,\n",
       "       154, 153, 152, 151, 150, 149, 148, 147, 146, 145, 144, 143, 142,\n",
       "       141, 140, 139, 138, 137, 136, 135, 134, 133, 132, 131, 130, 129,\n",
       "       128, 127, 126, 125, 124, 123, 122, 121, 120, 119, 118, 117, 116,\n",
       "       115, 114, 113, 112, 111, 110, 109, 108, 107, 106, 105, 104, 103,\n",
       "       102, 101, 100,  99,  98,  97,  96,  95,  94,  93,  92,  91,  90,\n",
       "        89,  88,  87,  86,  85,  84,  83,  82,  81,  80,  79,  78,  77,\n",
       "        76,  75,  74,  73,  72,  71,  70,  69,  68,  67,  66,  65,  64,\n",
       "        63,  62,  61,  60,  59,  58,  57,  56,  55,  54,  53,  52,  51,\n",
       "        50,  49,  48,  47,  46,  45,  44,  43,  42,  41,  40,  39,  38,\n",
       "        37,  36,  35,  34,  33,  32,  31,  30,  29,  28,  27,  26,  25,\n",
       "        24,  23,  22,  21,  20,  19,  18,  17,  16,  15,  14,  13,  12,\n",
       "        11,  10,   9,   8,   7,   6,   5,   4,   3,   2,   1,   0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(g())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea to make it faster\n",
    "\n",
    "1. each insert takes 5 times more time than just assigining value. because \"insert\" method inserts in some ranodm position \n",
    "2. same is not true for append. appends always inserts at the end\n",
    "3. so we only need to replace insert method with something else\n",
    "4. if we can find something where ordering or sequencing does not matter, this will be resolved\n",
    "5. instead of points we can work with segment length only. and corresponding flag and probability. Like the table bellow\n",
    "\n",
    "| length  | flag  | probability  |\n",
    "|---|---|---|\n",
    "|   |   |   |\n",
    "|   |   |   |  \n",
    "|   |   |   | \n",
    "\n",
    "7. Probably this can be made even faster by not using append in each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance\n",
    "| method  | time\n",
    "|---|---|\n",
    "realization_value    | $1.19$ s\n",
    "realization_value_v2 | $241$ ms\n",
    "\n",
    "realization_value_v2 is $~5$ times faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "1. slope with realization_value    $0.779388458353189$\n",
    "2. slope with realization_value_v2 $0.7841054354669876$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
