{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, numpy as np, pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "alphap = 2\n",
    "probp = 0.75\n",
    "expon = 2 * alphap - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def betadist(alpha):\n",
    "    \"\"\"gives a random number from beta distribution\"\"\"\n",
    "    return random.betavariate(alpha,alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision(probability):\n",
    "    \"\"\"\n",
    "    decides with a given probability whether to keep the right part\n",
    "    \"\"\"\n",
    "    if float(probability) > random.random():\n",
    "        return True\n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting(left,right):\n",
    "    \"\"\"\n",
    "    splits a given segment. left and right are endpoints of the segment\n",
    "    \"\"\"\n",
    "    segment = right - left\n",
    "    xL = segment * betadist(alphap)\n",
    "    xR = segment - xL\n",
    "    splitpoint = left + xL\n",
    "    flag = decision(probp)\n",
    "    xLp = xL**expon\n",
    "    xRp = xR**expon\n",
    "    change = xLp + xRp - segment**expon\n",
    "    return splitpoint, flag, xLp, xRp, change\n",
    "\n",
    "def splitting_v2(segment):\n",
    "    \"\"\"\n",
    "    splits a given segment. left and right are endpoints of the segment\n",
    "    returns : \n",
    "        xL -> length of the left segment\n",
    "        xR -> length of the right segment\n",
    "        flag -> keeping the right segment\n",
    "        xLp, xRp -> probability(unnormalized) for being selected\n",
    "        change -> change of normalization const\n",
    "    \"\"\"\n",
    "    xL = segment * betadist(alphap)\n",
    "    xR = segment - xL\n",
    "    flag = decision(probp)\n",
    "    xLp = xL**expon\n",
    "    xRp = xR**expon\n",
    "    change = xLp + xRp - segment**expon\n",
    "    return xL, xR, flag, xLp, xRp, change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickindex(expons, exponsum):\n",
    "    \"\"\"\n",
    "    picks up a segment to be subsequently split\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, exponsum)\n",
    "    sum_ = 0\n",
    "    for index in range(len(expons)):\n",
    "        sum_ += expons[index]\n",
    "        if sum_ < r:\n",
    "            continue\n",
    "        else:\n",
    "            return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_length(points,flags):\n",
    "    \n",
    "    N = 0\n",
    "    M = 0\n",
    "    \n",
    "    for i in range(len(flags)):\n",
    "        if flags[i]:\n",
    "            N += 1\n",
    "            M += points[i+1] - points[i] \n",
    "            pass\n",
    "        pass\n",
    "    return N, M\n",
    "\n",
    "def number_length_v2(lengths,flags):\n",
    "    \n",
    "    N = 0\n",
    "    M = 0\n",
    "#     print(flags)\n",
    "#     print(lengths)\n",
    "    for i in range(len(flags)):\n",
    "        if flags[i]:\n",
    "            N += 1\n",
    "            M += lengths[i]\n",
    "            pass\n",
    "        pass\n",
    "    return N, M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def realization_value(total_iteration, min_iteration, iteration_step):\n",
    "    \n",
    "    points = [0.,1.]\n",
    "    flags = [True]\n",
    "    expons = [1.]\n",
    "    exponsum = 1.0\n",
    "\n",
    "    iteration_list = list(range(min_iteration, total_iteration + 1, iteration_step))\n",
    "    N_realization = []\n",
    "    M_realization = []\n",
    "    \n",
    "    for i in range(total_iteration + 1):\n",
    "        \n",
    "        index = pickindex(expons, exponsum)\n",
    "        \n",
    "        if flags[index] == True:\n",
    "            left = points[index]\n",
    "            right = points[index+1]\n",
    "            splitpoint, flag, xLp, xRp, change = splitting(left,right)\n",
    "            points.insert(index+1,splitpoint)\n",
    "            flags.insert(index+1,flag)\n",
    "            expons[index] = xLp \n",
    "            expons.insert(index+1,xRp)\n",
    "            exponsum += change\n",
    "            pass\n",
    "        \n",
    "        if i+1 in iteration_list:\n",
    "            N, M = number_length(points,flags)\n",
    "            N_realization.append(N)\n",
    "            M_realization.append(M)\n",
    "        pass\n",
    "    \n",
    "    N_list = np.array(N_realization)\n",
    "    M_list = np.array(M_realization)\n",
    "    \n",
    "    return N_list, M_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def realization_value_v2(total_iteration, min_iteration, iteration_step):\n",
    "    \n",
    "    points = [0.,1.]\n",
    "    lengths = [1.]\n",
    "    flags = [True]\n",
    "    probabilities = [1.] # raw probability. not normalized\n",
    "    normC = 1.0 # normalization const\n",
    "\n",
    "    iteration_list = list(range(min_iteration, total_iteration + 1, iteration_step))\n",
    "    N_realization = []\n",
    "    M_realization = []\n",
    "    \n",
    "    for i in range(total_iteration + 1):\n",
    "        \n",
    "        index = pickindex(probabilities, normC)\n",
    "        \n",
    "        if flags[index] == True:\n",
    "\n",
    "            xL, xR, flag, xLp, xRp, change = splitting_v2(lengths[index])\n",
    "            \n",
    "            lengths[index] = xL\n",
    "            lengths.append(xR)\n",
    "            flags.append(flag)\n",
    "            probabilities[index] = xLp \n",
    "            probabilities.append(xRp)\n",
    "            normC += change\n",
    "            pass\n",
    "        \n",
    "        if i+1 in iteration_list:\n",
    "            N, M = number_length_v2(lengths,flags)\n",
    "            N_realization.append(N)\n",
    "            M_realization.append(M)\n",
    "        pass\n",
    "    \n",
    "    N_list = np.array(N_realization)\n",
    "    M_list = np.array(M_realization)\n",
    "    \n",
    "    return N_list, M_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241 ms ± 87.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# realization_value_v2(10, 2, 2)\n",
    "%timeit realization_value_v2(100000, 50000, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 8.82 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1.19 s ± 845 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit realization_value(100000, 50000, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_average(total_iteration = 1000, min_iteration = 100, iteration_step = 100, ensemble_size = 1000):\n",
    "\n",
    "    data_points = int ((total_iteration - min_iteration)/iteration_step + 1)\n",
    "    N_ensemble = np.zeros(data_points)\n",
    "    M_ensemble = np.zeros(data_points)\n",
    "    \n",
    "    for i in range(ensemble_size):\n",
    "        if i % 100 == 0:\n",
    "            print(\"iteration \", i)\n",
    "        N_list, M_list = realization_value(total_iteration, min_iteration, iteration_step)\n",
    "#         N_list, M_list = realization_value_v2(total_iteration, min_iteration, iteration_step)\n",
    "        N_ensemble += N_list\n",
    "        M_ensemble += M_list\n",
    "        pass\n",
    "    \n",
    "    N_average = N_ensemble/ensemble_size\n",
    "    M_average = M_ensemble/ensemble_size\n",
    "    \n",
    "    return N_average, M_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step i =  0\n",
      "step i =  0\n",
      "step i =  0\n",
      "step i =  0\n",
      "step i =  0\n",
      "step i =  0\n",
      "step i =  0\n",
      "step i =  0\n",
      "step i =  0\n",
      "step i =  0\n",
      "0.779388458353189\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_average, M_average = ensemble_average(10_000, 1_000, 1_000, 1_000)\n",
    "N_log = np.log(N_average)\n",
    "minus_delta_log = N_log - np.log(M_average)\n",
    "slope, intercept = np.polyfit(minus_delta_log, N_log, 1)\n",
    "print(slope)\n",
    "plt.plot(minus_delta_log, N_log, \"o\")\n",
    "plt.xlabel(\"-ln delta\")\n",
    "plt.ylabel(\"ln N\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f():\n",
    "    arr = [   ]\n",
    "    for a in range(1000):\n",
    "        arr.insert(0, a)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685 µs ± 10.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g():\n",
    "    arr = [0]*1000\n",
    "    i = 1000-1\n",
    "    for a in range(1000):\n",
    "        arr[i] = a\n",
    "        i -= 1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129 µs ± 3.07 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit g()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.310077519379845"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "685/129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([999, 998, 997, 996, 995, 994, 993, 992, 991, 990, 989, 988, 987,\n",
       "       986, 985, 984, 983, 982, 981, 980, 979, 978, 977, 976, 975, 974,\n",
       "       973, 972, 971, 970, 969, 968, 967, 966, 965, 964, 963, 962, 961,\n",
       "       960, 959, 958, 957, 956, 955, 954, 953, 952, 951, 950, 949, 948,\n",
       "       947, 946, 945, 944, 943, 942, 941, 940, 939, 938, 937, 936, 935,\n",
       "       934, 933, 932, 931, 930, 929, 928, 927, 926, 925, 924, 923, 922,\n",
       "       921, 920, 919, 918, 917, 916, 915, 914, 913, 912, 911, 910, 909,\n",
       "       908, 907, 906, 905, 904, 903, 902, 901, 900, 899, 898, 897, 896,\n",
       "       895, 894, 893, 892, 891, 890, 889, 888, 887, 886, 885, 884, 883,\n",
       "       882, 881, 880, 879, 878, 877, 876, 875, 874, 873, 872, 871, 870,\n",
       "       869, 868, 867, 866, 865, 864, 863, 862, 861, 860, 859, 858, 857,\n",
       "       856, 855, 854, 853, 852, 851, 850, 849, 848, 847, 846, 845, 844,\n",
       "       843, 842, 841, 840, 839, 838, 837, 836, 835, 834, 833, 832, 831,\n",
       "       830, 829, 828, 827, 826, 825, 824, 823, 822, 821, 820, 819, 818,\n",
       "       817, 816, 815, 814, 813, 812, 811, 810, 809, 808, 807, 806, 805,\n",
       "       804, 803, 802, 801, 800, 799, 798, 797, 796, 795, 794, 793, 792,\n",
       "       791, 790, 789, 788, 787, 786, 785, 784, 783, 782, 781, 780, 779,\n",
       "       778, 777, 776, 775, 774, 773, 772, 771, 770, 769, 768, 767, 766,\n",
       "       765, 764, 763, 762, 761, 760, 759, 758, 757, 756, 755, 754, 753,\n",
       "       752, 751, 750, 749, 748, 747, 746, 745, 744, 743, 742, 741, 740,\n",
       "       739, 738, 737, 736, 735, 734, 733, 732, 731, 730, 729, 728, 727,\n",
       "       726, 725, 724, 723, 722, 721, 720, 719, 718, 717, 716, 715, 714,\n",
       "       713, 712, 711, 710, 709, 708, 707, 706, 705, 704, 703, 702, 701,\n",
       "       700, 699, 698, 697, 696, 695, 694, 693, 692, 691, 690, 689, 688,\n",
       "       687, 686, 685, 684, 683, 682, 681, 680, 679, 678, 677, 676, 675,\n",
       "       674, 673, 672, 671, 670, 669, 668, 667, 666, 665, 664, 663, 662,\n",
       "       661, 660, 659, 658, 657, 656, 655, 654, 653, 652, 651, 650, 649,\n",
       "       648, 647, 646, 645, 644, 643, 642, 641, 640, 639, 638, 637, 636,\n",
       "       635, 634, 633, 632, 631, 630, 629, 628, 627, 626, 625, 624, 623,\n",
       "       622, 621, 620, 619, 618, 617, 616, 615, 614, 613, 612, 611, 610,\n",
       "       609, 608, 607, 606, 605, 604, 603, 602, 601, 600, 599, 598, 597,\n",
       "       596, 595, 594, 593, 592, 591, 590, 589, 588, 587, 586, 585, 584,\n",
       "       583, 582, 581, 580, 579, 578, 577, 576, 575, 574, 573, 572, 571,\n",
       "       570, 569, 568, 567, 566, 565, 564, 563, 562, 561, 560, 559, 558,\n",
       "       557, 556, 555, 554, 553, 552, 551, 550, 549, 548, 547, 546, 545,\n",
       "       544, 543, 542, 541, 540, 539, 538, 537, 536, 535, 534, 533, 532,\n",
       "       531, 530, 529, 528, 527, 526, 525, 524, 523, 522, 521, 520, 519,\n",
       "       518, 517, 516, 515, 514, 513, 512, 511, 510, 509, 508, 507, 506,\n",
       "       505, 504, 503, 502, 501, 500, 499, 498, 497, 496, 495, 494, 493,\n",
       "       492, 491, 490, 489, 488, 487, 486, 485, 484, 483, 482, 481, 480,\n",
       "       479, 478, 477, 476, 475, 474, 473, 472, 471, 470, 469, 468, 467,\n",
       "       466, 465, 464, 463, 462, 461, 460, 459, 458, 457, 456, 455, 454,\n",
       "       453, 452, 451, 450, 449, 448, 447, 446, 445, 444, 443, 442, 441,\n",
       "       440, 439, 438, 437, 436, 435, 434, 433, 432, 431, 430, 429, 428,\n",
       "       427, 426, 425, 424, 423, 422, 421, 420, 419, 418, 417, 416, 415,\n",
       "       414, 413, 412, 411, 410, 409, 408, 407, 406, 405, 404, 403, 402,\n",
       "       401, 400, 399, 398, 397, 396, 395, 394, 393, 392, 391, 390, 389,\n",
       "       388, 387, 386, 385, 384, 383, 382, 381, 380, 379, 378, 377, 376,\n",
       "       375, 374, 373, 372, 371, 370, 369, 368, 367, 366, 365, 364, 363,\n",
       "       362, 361, 360, 359, 358, 357, 356, 355, 354, 353, 352, 351, 350,\n",
       "       349, 348, 347, 346, 345, 344, 343, 342, 341, 340, 339, 338, 337,\n",
       "       336, 335, 334, 333, 332, 331, 330, 329, 328, 327, 326, 325, 324,\n",
       "       323, 322, 321, 320, 319, 318, 317, 316, 315, 314, 313, 312, 311,\n",
       "       310, 309, 308, 307, 306, 305, 304, 303, 302, 301, 300, 299, 298,\n",
       "       297, 296, 295, 294, 293, 292, 291, 290, 289, 288, 287, 286, 285,\n",
       "       284, 283, 282, 281, 280, 279, 278, 277, 276, 275, 274, 273, 272,\n",
       "       271, 270, 269, 268, 267, 266, 265, 264, 263, 262, 261, 260, 259,\n",
       "       258, 257, 256, 255, 254, 253, 252, 251, 250, 249, 248, 247, 246,\n",
       "       245, 244, 243, 242, 241, 240, 239, 238, 237, 236, 235, 234, 233,\n",
       "       232, 231, 230, 229, 228, 227, 226, 225, 224, 223, 222, 221, 220,\n",
       "       219, 218, 217, 216, 215, 214, 213, 212, 211, 210, 209, 208, 207,\n",
       "       206, 205, 204, 203, 202, 201, 200, 199, 198, 197, 196, 195, 194,\n",
       "       193, 192, 191, 190, 189, 188, 187, 186, 185, 184, 183, 182, 181,\n",
       "       180, 179, 178, 177, 176, 175, 174, 173, 172, 171, 170, 169, 168,\n",
       "       167, 166, 165, 164, 163, 162, 161, 160, 159, 158, 157, 156, 155,\n",
       "       154, 153, 152, 151, 150, 149, 148, 147, 146, 145, 144, 143, 142,\n",
       "       141, 140, 139, 138, 137, 136, 135, 134, 133, 132, 131, 130, 129,\n",
       "       128, 127, 126, 125, 124, 123, 122, 121, 120, 119, 118, 117, 116,\n",
       "       115, 114, 113, 112, 111, 110, 109, 108, 107, 106, 105, 104, 103,\n",
       "       102, 101, 100,  99,  98,  97,  96,  95,  94,  93,  92,  91,  90,\n",
       "        89,  88,  87,  86,  85,  84,  83,  82,  81,  80,  79,  78,  77,\n",
       "        76,  75,  74,  73,  72,  71,  70,  69,  68,  67,  66,  65,  64,\n",
       "        63,  62,  61,  60,  59,  58,  57,  56,  55,  54,  53,  52,  51,\n",
       "        50,  49,  48,  47,  46,  45,  44,  43,  42,  41,  40,  39,  38,\n",
       "        37,  36,  35,  34,  33,  32,  31,  30,  29,  28,  27,  26,  25,\n",
       "        24,  23,  22,  21,  20,  19,  18,  17,  16,  15,  14,  13,  12,\n",
       "        11,  10,   9,   8,   7,   6,   5,   4,   3,   2,   1,   0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(f())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([999, 998, 997, 996, 995, 994, 993, 992, 991, 990, 989, 988, 987,\n",
       "       986, 985, 984, 983, 982, 981, 980, 979, 978, 977, 976, 975, 974,\n",
       "       973, 972, 971, 970, 969, 968, 967, 966, 965, 964, 963, 962, 961,\n",
       "       960, 959, 958, 957, 956, 955, 954, 953, 952, 951, 950, 949, 948,\n",
       "       947, 946, 945, 944, 943, 942, 941, 940, 939, 938, 937, 936, 935,\n",
       "       934, 933, 932, 931, 930, 929, 928, 927, 926, 925, 924, 923, 922,\n",
       "       921, 920, 919, 918, 917, 916, 915, 914, 913, 912, 911, 910, 909,\n",
       "       908, 907, 906, 905, 904, 903, 902, 901, 900, 899, 898, 897, 896,\n",
       "       895, 894, 893, 892, 891, 890, 889, 888, 887, 886, 885, 884, 883,\n",
       "       882, 881, 880, 879, 878, 877, 876, 875, 874, 873, 872, 871, 870,\n",
       "       869, 868, 867, 866, 865, 864, 863, 862, 861, 860, 859, 858, 857,\n",
       "       856, 855, 854, 853, 852, 851, 850, 849, 848, 847, 846, 845, 844,\n",
       "       843, 842, 841, 840, 839, 838, 837, 836, 835, 834, 833, 832, 831,\n",
       "       830, 829, 828, 827, 826, 825, 824, 823, 822, 821, 820, 819, 818,\n",
       "       817, 816, 815, 814, 813, 812, 811, 810, 809, 808, 807, 806, 805,\n",
       "       804, 803, 802, 801, 800, 799, 798, 797, 796, 795, 794, 793, 792,\n",
       "       791, 790, 789, 788, 787, 786, 785, 784, 783, 782, 781, 780, 779,\n",
       "       778, 777, 776, 775, 774, 773, 772, 771, 770, 769, 768, 767, 766,\n",
       "       765, 764, 763, 762, 761, 760, 759, 758, 757, 756, 755, 754, 753,\n",
       "       752, 751, 750, 749, 748, 747, 746, 745, 744, 743, 742, 741, 740,\n",
       "       739, 738, 737, 736, 735, 734, 733, 732, 731, 730, 729, 728, 727,\n",
       "       726, 725, 724, 723, 722, 721, 720, 719, 718, 717, 716, 715, 714,\n",
       "       713, 712, 711, 710, 709, 708, 707, 706, 705, 704, 703, 702, 701,\n",
       "       700, 699, 698, 697, 696, 695, 694, 693, 692, 691, 690, 689, 688,\n",
       "       687, 686, 685, 684, 683, 682, 681, 680, 679, 678, 677, 676, 675,\n",
       "       674, 673, 672, 671, 670, 669, 668, 667, 666, 665, 664, 663, 662,\n",
       "       661, 660, 659, 658, 657, 656, 655, 654, 653, 652, 651, 650, 649,\n",
       "       648, 647, 646, 645, 644, 643, 642, 641, 640, 639, 638, 637, 636,\n",
       "       635, 634, 633, 632, 631, 630, 629, 628, 627, 626, 625, 624, 623,\n",
       "       622, 621, 620, 619, 618, 617, 616, 615, 614, 613, 612, 611, 610,\n",
       "       609, 608, 607, 606, 605, 604, 603, 602, 601, 600, 599, 598, 597,\n",
       "       596, 595, 594, 593, 592, 591, 590, 589, 588, 587, 586, 585, 584,\n",
       "       583, 582, 581, 580, 579, 578, 577, 576, 575, 574, 573, 572, 571,\n",
       "       570, 569, 568, 567, 566, 565, 564, 563, 562, 561, 560, 559, 558,\n",
       "       557, 556, 555, 554, 553, 552, 551, 550, 549, 548, 547, 546, 545,\n",
       "       544, 543, 542, 541, 540, 539, 538, 537, 536, 535, 534, 533, 532,\n",
       "       531, 530, 529, 528, 527, 526, 525, 524, 523, 522, 521, 520, 519,\n",
       "       518, 517, 516, 515, 514, 513, 512, 511, 510, 509, 508, 507, 506,\n",
       "       505, 504, 503, 502, 501, 500, 499, 498, 497, 496, 495, 494, 493,\n",
       "       492, 491, 490, 489, 488, 487, 486, 485, 484, 483, 482, 481, 480,\n",
       "       479, 478, 477, 476, 475, 474, 473, 472, 471, 470, 469, 468, 467,\n",
       "       466, 465, 464, 463, 462, 461, 460, 459, 458, 457, 456, 455, 454,\n",
       "       453, 452, 451, 450, 449, 448, 447, 446, 445, 444, 443, 442, 441,\n",
       "       440, 439, 438, 437, 436, 435, 434, 433, 432, 431, 430, 429, 428,\n",
       "       427, 426, 425, 424, 423, 422, 421, 420, 419, 418, 417, 416, 415,\n",
       "       414, 413, 412, 411, 410, 409, 408, 407, 406, 405, 404, 403, 402,\n",
       "       401, 400, 399, 398, 397, 396, 395, 394, 393, 392, 391, 390, 389,\n",
       "       388, 387, 386, 385, 384, 383, 382, 381, 380, 379, 378, 377, 376,\n",
       "       375, 374, 373, 372, 371, 370, 369, 368, 367, 366, 365, 364, 363,\n",
       "       362, 361, 360, 359, 358, 357, 356, 355, 354, 353, 352, 351, 350,\n",
       "       349, 348, 347, 346, 345, 344, 343, 342, 341, 340, 339, 338, 337,\n",
       "       336, 335, 334, 333, 332, 331, 330, 329, 328, 327, 326, 325, 324,\n",
       "       323, 322, 321, 320, 319, 318, 317, 316, 315, 314, 313, 312, 311,\n",
       "       310, 309, 308, 307, 306, 305, 304, 303, 302, 301, 300, 299, 298,\n",
       "       297, 296, 295, 294, 293, 292, 291, 290, 289, 288, 287, 286, 285,\n",
       "       284, 283, 282, 281, 280, 279, 278, 277, 276, 275, 274, 273, 272,\n",
       "       271, 270, 269, 268, 267, 266, 265, 264, 263, 262, 261, 260, 259,\n",
       "       258, 257, 256, 255, 254, 253, 252, 251, 250, 249, 248, 247, 246,\n",
       "       245, 244, 243, 242, 241, 240, 239, 238, 237, 236, 235, 234, 233,\n",
       "       232, 231, 230, 229, 228, 227, 226, 225, 224, 223, 222, 221, 220,\n",
       "       219, 218, 217, 216, 215, 214, 213, 212, 211, 210, 209, 208, 207,\n",
       "       206, 205, 204, 203, 202, 201, 200, 199, 198, 197, 196, 195, 194,\n",
       "       193, 192, 191, 190, 189, 188, 187, 186, 185, 184, 183, 182, 181,\n",
       "       180, 179, 178, 177, 176, 175, 174, 173, 172, 171, 170, 169, 168,\n",
       "       167, 166, 165, 164, 163, 162, 161, 160, 159, 158, 157, 156, 155,\n",
       "       154, 153, 152, 151, 150, 149, 148, 147, 146, 145, 144, 143, 142,\n",
       "       141, 140, 139, 138, 137, 136, 135, 134, 133, 132, 131, 130, 129,\n",
       "       128, 127, 126, 125, 124, 123, 122, 121, 120, 119, 118, 117, 116,\n",
       "       115, 114, 113, 112, 111, 110, 109, 108, 107, 106, 105, 104, 103,\n",
       "       102, 101, 100,  99,  98,  97,  96,  95,  94,  93,  92,  91,  90,\n",
       "        89,  88,  87,  86,  85,  84,  83,  82,  81,  80,  79,  78,  77,\n",
       "        76,  75,  74,  73,  72,  71,  70,  69,  68,  67,  66,  65,  64,\n",
       "        63,  62,  61,  60,  59,  58,  57,  56,  55,  54,  53,  52,  51,\n",
       "        50,  49,  48,  47,  46,  45,  44,  43,  42,  41,  40,  39,  38,\n",
       "        37,  36,  35,  34,  33,  32,  31,  30,  29,  28,  27,  26,  25,\n",
       "        24,  23,  22,  21,  20,  19,  18,  17,  16,  15,  14,  13,  12,\n",
       "        11,  10,   9,   8,   7,   6,   5,   4,   3,   2,   1,   0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(g())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea to make it faster\n",
    "\n",
    "1. each insert takes 5 times more time than just assigining value. because \"insert\" method inserts in some ranodm position \n",
    "2. same is not true for append. appends always inserts at the end\n",
    "3. so we only need to replace insert method with something else\n",
    "4. if we can find something where ordering or sequencing does not matter, this will be resolved\n",
    "5. instead of points we can work with segment length only. and corresponding flag and probability. Like the table bellow\n",
    "\n",
    "| length  | flag  | probability  |\n",
    "|---|---|---|\n",
    "|   |   |   |\n",
    "|   |   |   |  \n",
    "|   |   |   | \n",
    "\n",
    "7. Probably this can be made even faster by not using append in each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance\n",
    "| method  | time\n",
    "|---|---|\n",
    "realization_value    | $1.19$ s\n",
    "realization_value_v2 | $241$ ms\n",
    "\n",
    "realization_value_v2 is $~5$ times faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "1. slope with realization_value    $0.779388458353189$\n",
    "2. slope with realization_value_v2 $0.7841054354669876$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
